{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit False Information Detection\n",
    "\n",
    "## Background and current situation\n",
    "\n",
    "### Goals\n",
    "In this project, we want to find out if if an algorithm trained on existing fake news datasets is suitable for detecting misinformation in the quite specific type of content posted on Reddit. To assess the predictions made by our Machine Learning model, we check the online status of the streamed content exactly one week after it was posted. Our four areas of interest and main questions / goals are:\n",
    "\n",
    "- Content moderation: determine if our method can be effective in filtering out misinformation compared to human content moderation\n",
    "- Resource use: is our method efficient and scalable for real-time Big Data analysis?\n",
    "- Reddit insights: analyze posts / comments classified as misinformation\n",
    "- Model applicability: find out if an algorithm trained on existing misinformation datasets is suitable for Reddit\n",
    "\n",
    "### Notebook structure\n",
    "- reddit.ipynb: This notebook contains everything related to getting data from the Reddit API (streaming real-time content, retrieving data 1 week later), applying a trained model for classification, calculating performance metrics and some statistics as well as analyzing and discussing the results.\n",
    "- ml_model_training[...].ipynb: The code and information related to the training of the ML models can be found in two separate notebooks (one for the model trained on the Truthseeker dataset, one for that trained on the Fakeddit dataset).\n",
    "\n",
    "### Reddit API changes\n",
    "Reddit has had a free API for 7 years now, however in April 2023 they announced that changes will come to their API. The result of the changes would be that third party apps that access Redditâ€™s API will need to pay on a request basis going forward. Therefore, by the end of June third party apps will either be shut down or need to pay $0.24 per 1.000 API calls. However, Reddit says that apps using less than 100 requests per minute through the OAuth client ID will continue to be able to use the API free of charge. One major remaining question is what Reddit classifies as \"one request\" - for example, with the existing information, it is not clear to us if continuous streaming (like in our project) would be counted as one request or if each streamed submission or comment appearing in the stream would be counted as one request. Since it was critically important for us to finish all of our tasks that access the API before the changes apply and the exact date was not clear (19th of June or 1st of July were the two possible dates), we did all of these parts before June 19th, just to be on the safe side. \n",
    "The official reason given by Reddit for the API changes was that users visiting the site through third-party apps may not see ads that Reddit serves on its website and first-party app, so it was not sustainable any longer for the company. In an â€œAsk me anythingâ€ by Reddit CEO Steve Huffman, it was announced that moderation tools that need API access will remain free of charge.\n",
    "\n",
    "### Protests and blackout\n",
    "Almost 8.000 subreddits participated in an event called â€œReddarkâ€, where the moderators put the communities into private mode, so users could no longer access the specific subreddits. The blackout started on June 12th and the protest was aimed to last at least 48 hours, including some of the biggest subreddits on the platform. In total, subreddits participating in Reddark had a total of two and a half billion members (cumulative, not unique users).\n",
    "Over 6.000 subreddits stayed offline even after the 48 hours:\n",
    "\n",
    "- Affected subreddits with 40+ million users: r/funny\n",
    "- Affected subreddits with 30+ million users: r/aww, r/gaming, r/Music, r/Pics, r/science, r/todayilearned\n",
    "- Affected subreddits with 20+ million users: r/art, r/askscience, r/books, r/DIY, r/EarthPorn, r/explainlikeimfive, r/food, r/gadgets, r/gifs, r/LifeProTips, r/memes, r/mildlyinteresting, r/NotTheOnion, r/Showerthoughts, r/space, r/sports, r/videos\n",
    "\n",
    "An internal company memo was leaked which told employees that the blackouts did not really affect revenue and that the company's aim is to sit them out instead of responding to them. \n",
    "\n",
    "### Subreddit rules and moderation\n",
    "In our selected subreddits (r/news, r/politics, r/science, r/worldnews), the number of submissions is much lower than the number of comments (roughly by a factor of 100). One possible reason for this extreme difference could be the subbredit-specific level of moderation and the sometimes very strict rules for submissions. For example, in r/politics there is a set of domains for submissions which are approved, if a submission is not from the approved domains list it gets removed. Also, new accounts that have not reached a certain time on the platform yet have restricted access to posting. Even more, the submission title on Reddit really must have the exact same wording as the news article headline that is linked. Also, additional text below the title is often prohibited, so adding opinions to the submission leads to a removal because only original text out of the article is allowed in the submission. In addition, the news article that gets submitted must not be older than 1 week of the time posting.\n",
    "\n",
    "\n",
    "### Sources:\n",
    "\n",
    "https://www.businessinsider.com/biggest-subreddits-affected-by-48-hour-blackout-list-private-2023-6 \n",
    "\n",
    "https://www.digitaltrends.com/computing/reddit-api-changes-explained/ \n",
    "\n",
    "https://www.forbes.com/sites/antoniopequenoiv/2023/06/13/reddit-stands-by-controversial-api-changes-as-subreddit-protest-continues/ \n",
    "\n",
    "https://www.reddit.com/r/news/ \n",
    "\n",
    "https://www.reddit.com/r/politics/ \n",
    "\n",
    "https://www.reddit.com/r/science/  \n",
    "\n",
    "https://www.reddit.com/r/worldnews/ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming and classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import spacy_sentence_bert\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Reddit API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit's API is used via the Python package \"PRAW\". The client ID and secret were created after registering a Reddit account and registering an app at https://www.reddit.com/prefs/apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='...',\n",
    "                     client_secret='...',\n",
    "                     user_agent='...',\n",
    "                     username='...',\n",
    "                     password='...')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming all new posts on Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_start = time()\n",
    "for submission in reddit.subreddit('all').stream.submissions():\n",
    "    if submission.created_utc > stream_start:\n",
    "        print('id: ' + str(submission.id))\n",
    "        print('unix_timestamp: ' + str(submission.created_utc))\n",
    "        print('subreddit: ' + str(submission.subreddit.display_name))\n",
    "        print('author_name: ' + str(submission.author))\n",
    "        print('author_id: ' + str(submission.author.id))\n",
    "        print('upvotes: ' + str(submission.score))\n",
    "        print('title: ' + str(submission.title))\n",
    "        print('url: ' + str(submission.url))\n",
    "        print('content: ' + str(submission.selftext))\n",
    "        print('________________________________________________ \\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming all new comments on Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_start = time()\n",
    "for comment in reddit.subreddit('all').stream.comments():\n",
    "    if comment.created_utc > stream_start:\n",
    "        print('id: ' + str(comment.id))\n",
    "        print('unix_timestamp: ' + str(comment.created_utc))\n",
    "        print('subreddit: ' + str(comment.subreddit.display_name))\n",
    "        print('submission_id: ' + str(comment.submission.id))\n",
    "        print('author_name: ' + str(comment.author))\n",
    "        print('author_id: ' + str(comment.author.id))\n",
    "        print('upvotes: ' + str(comment.score))\n",
    "        print('content: ' + str(comment.body))\n",
    "        print('________________________________________________ \\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification (applying the ML model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are defining a function that loads the trained ML model, applies it to Reddit data to classify it as misinformation or truthful content and saves the streamed data and the classification label to a CSV file.\n",
    "\n",
    "Note: the \"classify_headline\" function was initially applying the ML model on a wrong column (\"selftext\" instead of \"title\"), so the labels for the submissions streamed in real time were wrong. We noticed and corrected this mistake later, repeated the classficiations and replaced the affected CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_headline(data, model = \"Twitter\"):\n",
    "    # Load library\n",
    "    import skops.io as sio\n",
    "\n",
    "    # Load persisted model\n",
    "    if model == \"Twitter\":\n",
    "        trained_model = sio.load(file = \"models/statement_rf_model.skops\", trusted = True)\n",
    "    elif model == \"Reddit\":\n",
    "        nb_model = sio.load(file = \"models/fakeddit_nb_model.skops\", trusted = True)\n",
    "        nn_model = sio.load(file = \"models/fakeddit_nn_model.skops\", trusted = True)\n",
    "\n",
    "    if model == \"Twitter\":\n",
    "        # Vectorize content\n",
    "        nlp = spacy_sentence_bert.load_model('en_stsb_distilbert_base')\n",
    "        title = nlp(data[\"title\"][0]).vector.reshape(1, -1)\n",
    "    elif model == \"Reddit\":\n",
    "        title = [data[\"title\"]]\n",
    "\n",
    "    # Classify data\n",
    "    if model == \"Twitter\":\n",
    "        label = trained_model.predict(title)\n",
    "    elif model == \"Reddit\":\n",
    "        reddit_nb_label = nb_model.predict(title)\n",
    "        reddit_nn_label = nn_model.predict(title)\n",
    "\n",
    "    # Show message\n",
    "    # if(label == 1):\n",
    "    #     print(\"Nothing detected by the model. \\n\")\n",
    "    # else:\n",
    "    #     print(\"Misinformation detected! ðŸ‘ŽðŸ¼ \\n\")\n",
    "\n",
    "    # Add label to dataframe\n",
    "    if model == \"Twitter\":\n",
    "        data[\"label\"] = label\n",
    "    elif model == \"Reddit\":\n",
    "        data[\"reddit_nb_label\"] = reddit_nb_label[0]\n",
    "        data[\"reddit_nn_label\"] = reddit_nn_label[0]\n",
    "\n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used to to insert additional labels based on the model trained on Fakeddit data\n",
    "# We also used a variation of this code to fix the wrong labels in \"submissions_classified_[...].csv\" and \"submissions_detailed_[...].csv files\n",
    "# Export file names are only temporary to not overwrite input files\n",
    "\n",
    "# data = pd.read_csv(\"results/submissions_detailed_r_science.csv\")\n",
    "\n",
    "# Reddit code\n",
    "\n",
    "# data[\"reddit_nb_label\"] = pd.Series(dtype = \"int64\")\n",
    "# data[\"reddit_nn_label\"] = pd.Series(dtype = \"int64\")\n",
    "\n",
    "# for index, row in data.iterrows():\n",
    "#     data.loc[index] = classify_headline(row, model = \"Reddit\")\n",
    "\n",
    "# data[\"reddit_nb_label\"] = data[\"reddit_nb_label\"].astype(int)\n",
    "# data[\"reddit_nn_label\"] = data[\"reddit_nn_label\"].astype(int)\n",
    "\n",
    "# data.to_csv(\"results/submissions_detailed_FULL_r_science.csv\", index = None)\n",
    "\n",
    "# Code to fix erroneous labels\n",
    "\n",
    "# for index, row in data.iterrows():\n",
    "#     data.loc[index] = classify_headline(row, model = \"Reddit\")\n",
    "\n",
    "# data.to_csv(\"results/submissions_detailed_CORRECTED_r_news.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(data):\n",
    "    # Load library\n",
    "    import skops.io as sio\n",
    "\n",
    "    # Load persisted model\n",
    "    trained_model = sio.load(file = \"models/tweet_nn_model.skops\", trusted = True)\n",
    "\n",
    "    # Vectorize content\n",
    "    nlp = spacy_sentence_bert.load_model('en_stsb_distilbert_base')\n",
    "    content = nlp(data[\"content\"][0]).vector.reshape(1, -1)\n",
    "\n",
    "    # Classify data\n",
    "    label = trained_model.predict(content)\n",
    "\n",
    "    # Show message\n",
    "    # if(label == 1):\n",
    "    #     print(\"Nothing detected by the model. \\n\")\n",
    "    # else:\n",
    "    #     print(\"Misinformation detected! ðŸ‘ŽðŸ¼ \\n\")\n",
    "\n",
    "    # Add label to dataframe\n",
    "    data[\"label\"] = label\n",
    "\n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming and real-time classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_and_classify_submissions(subreddit):\n",
    "    # Load library\n",
    "    import os\n",
    "\n",
    "    # Set current time as start time of the stream\n",
    "    stream_start_unix_timestamp = time()\n",
    "    stream_start_timestamp = datetime.now().strftime(\"_%Y_%m_%d_%H_%M_%S\")\n",
    "    \n",
    "    # Loop through every new submission in the subreddit\n",
    "    for submission in reddit.subreddit(subreddit).stream.submissions():\n",
    "        if submission.created_utc > stream_start_unix_timestamp:\n",
    "            try:\n",
    "                # Get details\n",
    "                id = str(submission.id)\n",
    "                unix_timestamp = submission.created_utc\n",
    "                subreddit_name = str(submission.subreddit.display_name)\n",
    "                author_name = str(submission.author)\n",
    "                author_id = str(submission.author.id)\n",
    "                upvotes = submission.score\n",
    "                title = str(submission.title)\n",
    "                url = str(submission.url)\n",
    "                content = str(submission.selftext.splitlines())\n",
    "\n",
    "                # Print details\n",
    "                # print('id: ' + id)\n",
    "                # print('unix_timestamp: ' + str(unix_timestamp))\n",
    "                # print('subreddit_name: ' + subreddit_name)\n",
    "                # print('author_name: ' + author_name)\n",
    "                # print('author_id: ' + author_id)\n",
    "                # print('upvotes: ' + str(upvotes))\n",
    "                # print('title: ' + title)\n",
    "                # print('url: ' + url)\n",
    "                # print('content: ' + content)\n",
    "                # print('________________________________________________ \\n')\n",
    "\n",
    "                # Create dataframe\n",
    "                data = pd.DataFrame([[id, unix_timestamp, subreddit_name, author_name, author_id, upvotes, title, url, content]],\n",
    "                                    columns=[\"id\", \"unix_timestamp\", \"subreddit_name\", \"author_name\", \"author_id\", \"upvotes\", \"title\", \"url\", \"content\"])\n",
    "\n",
    "                # Classify data\n",
    "                classified_data = classify_headline(data)\n",
    "\n",
    "                # Save data: if a CSV exists already, append data without header, otherwise create new CSV with header\n",
    "                file_name = \"results/submissions_classified_\" + \"r_\" + subreddit + stream_start_timestamp + \".csv\"\n",
    "                classified_data.to_csv(file_name, mode=\"a\", index=False, header=not os.path.isfile(file_name))\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_and_classify_comments(subreddit):\n",
    "    # Load library\n",
    "    import os\n",
    "    \n",
    "    # Set current time as start time of the stream\n",
    "    stream_start_unix_timestamp = time()\n",
    "    stream_start_timestamp = datetime.now().strftime(\"_%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    # Loop through every new comment in the subreddit\n",
    "    for comment in reddit.subreddit(subreddit).stream.comments():\n",
    "        if comment.created_utc > stream_start_unix_timestamp:\n",
    "            try:\n",
    "                # Get details\n",
    "                id = str(comment.id)\n",
    "                unix_timestamp = comment.created_utc\n",
    "                subreddit_name = str(comment.subreddit.display_name)\n",
    "                submission_id = str(comment.submission.id)\n",
    "                author_name = str(comment.author)\n",
    "                author_id = str(comment.author.id)\n",
    "                upvotes = comment.score\n",
    "                content = str(comment.body.splitlines())\n",
    "\n",
    "                # Print details\n",
    "                # print('id: ' + id)\n",
    "                # print('unix_timestamp: ' + str(unix_timestamp))\n",
    "                # print('subreddit_name: ' + subreddit_name)\n",
    "                # print('submission_id: ' + submission_id)\n",
    "                # print('author_name: ' + author_name)\n",
    "                # print('author_id: ' + author_id)\n",
    "                # print('upvotes: ' + str(upvotes))\n",
    "                # print('content: ' + content)\n",
    "                # print('________________________________________________ \\n')\n",
    "\n",
    "                # Create dataframe\n",
    "                data = pd.DataFrame([[id, unix_timestamp, subreddit_name, submission_id, author_name, author_id, upvotes, content]],\n",
    "                                    columns=[\"id\", \"unix_timestamp\", \"subreddit_name\", \"submission_id\", \"author_name\", \"author_id\", \"upvotes\", \"content\"])\n",
    "\n",
    "                # Classify data\n",
    "                classified_data = classify_text(data)\n",
    "\n",
    "                # Save data: if a CSV exists already, append data without header, otherwise create new CSV with header\n",
    "                file_name = \"results/comments_classified_\" + \"r_\" + subreddit + stream_start_timestamp + \".csv\"\n",
    "                classified_data.to_csv(file_name, mode=\"a\", index=False, header=not os.path.isfile(file_name))\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce negative effects of any interruptions, each subreddit is streamed in a different notebook and the data is saved to unique CSVs with timestamps!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "### Getting details about streamed content by ID \n",
    "To check if the streamed content had been deleted or not, we had to retrieve all available details for each submission or comment. This was done exactly 1 week after the content had been streamed and classified. Using PRAW's info() function is ideal, since it requests the data in batches of 100 and not one by one. The function requires a list of IDs with the correct prefix (for submission or comment) added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick ONE import and export only from the options below - either submissions or comments from one subreddit!\n",
    "\"\"\"\n",
    "### Submissions only ###\n",
    "\n",
    "# Get submission IDs and labels\n",
    "# streamed_data = pd.read_csv(\"results/submissions_classified_r_news.csv\")\n",
    "# streamed_data = pd.read_csv(\"results/submissions_classified_r_politics.csv\")\n",
    "# streamed_data = pd.read_csv(\"results/submissions_classified_r_science.csv\")\n",
    "streamed_data = pd.read_csv(\"results/submissions_classified_r_worldnews.csv\")\n",
    "streamed_data = streamed_data[[\"id\", \"label\"]]\n",
    "\n",
    "# First submission ID\n",
    "first_full_id = [i if i.startswith('t3_') else f't3_{i}' for i in streamed_data.loc[:0,\"id\"]]\n",
    "\n",
    "# Rest of the submission IDs\n",
    "rest_full_ids = [i if i.startswith('t3_') else f't3_{i}' for i in streamed_data.loc[1:,\"id\"]]\n",
    "\"\"\"\n",
    "\n",
    "### Comments only ###\n",
    "\n",
    "# Get comment IDs and labels\n",
    "# streamed_data = pd.read_csv(\"results/comments_classified_r_news.csv\")\n",
    "# streamed_data = pd.read_csv(\"results/comments_classified_r_politics.csv\")\n",
    "# streamed_data = pd.read_csv(\"results/comments_classified_r_science.csv\")\n",
    "streamed_data = pd.read_csv(\"results/comments_classified_r_worldnews.csv\")\n",
    "streamed_data = streamed_data[[\"id\", \"label\"]]\n",
    "\n",
    "# First comment ID\n",
    "first_full_id = [i if i.startswith('t1_') else f't1_{i}' for i in streamed_data.loc[:0,\"id\"]]\n",
    "\n",
    "# Rest of the comment IDs\n",
    "rest_full_ids = [i if i.startswith('t1_') else f't1_{i}' for i in streamed_data.loc[1:,\"id\"]]\n",
    "\n",
    "### Submissions or comments ###\n",
    "\n",
    "# Get data about the first ID\n",
    "for submission_or_comment in reddit.info(fullnames = first_full_id):\n",
    "\tdata = vars(submission_or_comment)\n",
    "\tdf = pd.json_normalize(data)\n",
    "\n",
    "# Get data about the rest of the IDs\n",
    "for submission_or_comment in reddit.info(fullnames = rest_full_ids):\n",
    "\tdata = vars(submission_or_comment)\n",
    "\tdf_row = pd.json_normalize(data)\n",
    "\tdf = pd.concat([df, df_row], join = \"inner\", ignore_index = True)\n",
    "\n",
    "# Remove linebreaks from text\n",
    "if \"selftext\" in df.columns:\n",
    "\tdf[\"selftext\"] = df[\"selftext\"].apply(lambda row: str(row.splitlines()).strip(\"[]\") if row != None else row)\n",
    "\tdf[\"selftext_html\"] = df[\"selftext_html\"].apply(lambda row: str(row.splitlines()).strip(\"[]\") if row != None else row)\n",
    "elif \"body\" in df.columns:\n",
    "\tdf[\"body\"] = df[\"body\"].apply(lambda row: str(row.splitlines()).strip(\"[]\") if row != None else row)\n",
    "\tdf[\"body_html\"] = df[\"body_html\"].apply(lambda row: str(row.splitlines()).strip(\"[]\") if row != None else row)\n",
    "\n",
    "# Merge with streamed data to add classification label\n",
    "df = pd.merge(df, streamed_data, on = \"id\")\n",
    "\n",
    "# Move ID column to the first position\n",
    "df.insert(0, 'id', df.pop('id'))\n",
    "\n",
    "# Export as CSV\n",
    "# df.to_csv(\"results/submissions_detailed_r_news.csv\", index = False)\n",
    "# df.to_csv(\"results/submissions_detailed_r_politics.csv\", index = False)\n",
    "# df.to_csv(\"results/submissions_detailed_r_science.csv\", index = False)\n",
    "# df.to_csv(\"results/submissions_detailed_r_worldnews.csv\", index = False)\n",
    "# df.to_csv(\"results/comments_detailed_r_news.csv\", index = False)\n",
    "# df.to_csv(\"results/comments_detailed_r_politics.csv\", index = False)\n",
    "# df.to_csv(\"results/comments_detailed_r_science.csv\", index = False)\n",
    "# df.to_csv(\"results/comments_detailed_r_worldnews.csv\", index = False)\n",
    "\n",
    "# Display the entire content of the dataframe\n",
    "# from IPython.display import display, HTML\n",
    "# display(HTML(df.to_html(col_space = 300)))\n",
    "\n",
    "# Print\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal indicators and general notes\n",
    "\n",
    "<b>General:</b>\n",
    "\n",
    "\"[removed]\" indicates that the submission/comment was removed by a moderator of the subreddit (because it broke the rules). <br>\n",
    "\"[deleted]\" indicates that the submission/comment was deleted by its author. <br>\n",
    "\n",
    "<b>Comments:</b>\n",
    "\n",
    "If the author is undefined, but the comment is still online, the user account was removed or deleted!\n",
    "\n",
    "Fields that signal removal / deletion of the comment:\n",
    "* \"body\": \"[removed]\" / \"[deleted]\"\n",
    "\n",
    "Fields that signal controversiality of the comment:\n",
    "* \"score\": negative integer\n",
    "\n",
    "<b>Submissions:</b>\n",
    "\n",
    "If the author is undefined, the user account was deleted or removed!\n",
    "\n",
    "Fields that signal removal / deletion of the submission:\n",
    "* \"selftext\": \"[removed]\" / \"[deleted]\"\n",
    "* \"removed_by_category\": not \"nan\"\n",
    "* \"is_robot_indexable\": False\n",
    "\n",
    "Fields that signal controversiality of the submission:\n",
    "* \"upvote_ratio\": low value of float\n",
    "\n",
    "Check if the number of upvotes is preserved even when content is taken down!\n",
    "\n",
    "<b>Notes:</b>\n",
    "\n",
    "The absolute number of downvotes is not provided anymore even though the field \"downs\" still exists. (Confirmation: https://github.com/praw-dev/praw/issues/881). For submissions, the number of upvotes is given in the \"score\" attribute and the share of upvotes among all votes is given in the \"upvote_ratio\" column. For comments, the difference between the number of upvotes and the number of downvotes is given in the \"score\" column.\n",
    "\n",
    "<b>Important:</b> Some existing columns in Reddit API data are unused, others are used differently (contain different data) for submissions and comments!\n",
    "\n",
    "It's not possible to retrieve deleted or removed comments via the submission they responded to as only the IDs of comments that are still visible are included in the submission metadata. (Tested on this submission: https://www.reddit.com/r/redditdev/comments/geybhw/praw_is_there_any_way_to_determine_if_a_link_post/)\n",
    "\n",
    "Unfortunately, we cannot include posts being marked NSFW / \"over_18\" by moderators as misinformation indicators. While it is possible that moderators mark content as NSFW after it was postetd, users can also do this for their own content at the time of posting. As we did not save the \"over_18\" tag during the data streaming, it's now impossible for us to tell which of the two cases is applicable.\n",
    "\n",
    "To retrieve content via its ID using the info() function, submissions IDs must be prefixed with \"t3_\" and comment IDs with \"t1_\". (Source: https://www.reddit.com/dev/api/)\n",
    "\n",
    "<b>Reddit Blackout Notes:</b>\n",
    "\n",
    "On 2023-06-17, when fetching the detailed data for the streamed content one week after streaming:\n",
    "* r/news, r/politics, r/worldnews were available and active.\n",
    "* r/science was visible again after the protests starting on 2023-06-12, but was still restricted (= no new content allowed).\n",
    "\n",
    "Below are functions to categorize and analyze the submissions and comments.\n",
    "Note: the value \"truth\" for the prediction target below is a simplification, \"not_clearly_misinformation\" would be more accurate but also more inconvenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_submissions(df):\n",
    "    # Submission was removed / deleted\n",
    "    if (df[\"is_robot_indexable\"] == False) or (df[\"removed_by_category\"] != \"nan\"):\n",
    "        # Submission was removed by a moderator\n",
    "        if (df[\"removed_by_category\"] == \"moderator\") or (df[\"removed_by_category\"] == \"reddit\") or (df[\"removed_by_category\"] == \"deleted\" and df[\"selftext\"] == \"'[removed]'\"):\n",
    "            return \"removed_by_moderator\"\n",
    "        # Submission was deleted by the user\n",
    "        elif (df[\"removed_by_category\"] == \"deleted\" and df[\"selftext\"] == \"'[deleted]'\"):\n",
    "            return \"deleted_by_user\"\n",
    "    # Submission was not removed / deleted\n",
    "    else:\n",
    "        return \"online\"\n",
    "    \n",
    "\n",
    "def categorize_comments(df):\n",
    "    # Comment was removed by a moderator\n",
    "    if df[\"body\"] == \"'[removed]'\":\n",
    "        return \"removed_by_moderator\"\n",
    "    # Coment was deleted by the user\n",
    "    elif df[\"body\"] == \"'[deleted]'\":\n",
    "        return \"deleted_by_user\"\n",
    "    # Comment was not removed / deleted\n",
    "    else:\n",
    "        return \"online\"\n",
    "\n",
    "\n",
    "def analyze_submissions(path, prediction_target, model = \"Twitter\"):\n",
    "    # Import CSV file\n",
    "    df = pd.read_csv(path, low_memory = False)\n",
    "\n",
    "    # Convert column types\n",
    "    df[\"author\"] = df[\"author\"].apply(str)\n",
    "    df[\"removed_by_category\"] = df[\"removed_by_category\"].apply(str)\n",
    "\n",
    "    # Add column on removal status\n",
    "    df['removal_status'] = df.apply(categorize_submissions, axis = 1)\n",
    "\n",
    "    # Add column on online status\n",
    "    df.loc[df[\"removal_status\"] == \"removed_by_moderator\", \"online\"] = 0\n",
    "    df.loc[df[\"removal_status\"] == \"deleted_by_user\", \"online\"] = 0\n",
    "    df.loc[df[\"removal_status\"] == \"online\", \"online\"] = 1\n",
    "    df[\"online\"] = df[\"online\"].astype(int)\n",
    "\n",
    "    if model == \"Twitter\":\n",
    "        y_prediction = df[\"label\"]\n",
    "    elif model == \"Reddit NB\":\n",
    "        y_prediction = df[\"reddit_nb_label\"]\n",
    "    elif model == \"Reddit NN\":\n",
    "        y_prediction = df[\"reddit_nn_label\"]\n",
    "        \n",
    "    y_actual = df[\"online\"]\n",
    "\n",
    "    if prediction_target == \"misinformation\":\n",
    "        target_label = 0\n",
    "    elif prediction_target == \"truth\":\n",
    "        target_label = 1\n",
    "\n",
    "    # Calculate and print scores\n",
    "    print(f'Accuracy: {round(accuracy_score(y_actual, y_prediction) * 100, 2)}%')\n",
    "    print(f'Precision: {round(precision_score(y_actual, y_prediction, pos_label = target_label) * 100, 2)}%')\n",
    "    print(f'Recall: {round(recall_score(y_actual, y_prediction, pos_label = target_label) * 100, 2)}%')\n",
    "    print(f'F1 Score: {round(f1_score(y_actual, y_prediction, pos_label = target_label) * 100, 2)}%')\n",
    "\n",
    "    confusion_matrix_lr = pd.crosstab(y_prediction, y_actual)\n",
    "    display(confusion_matrix_lr)\n",
    "\n",
    "    # Show the unique values of every column\n",
    "    # for col in df:\n",
    "    #     print(col + \": \" + str(df[col].unique()))\n",
    "\n",
    "    # return df[[\"id\", \"author\", \"selftext\", \"score\", \"upvote_ratio\", \"removal_status\", \"online\", \"label\"]]\n",
    "     \n",
    "\n",
    "def analyze_comments(path, prediction_target):\n",
    "    # Import CSV file\n",
    "    df = pd.read_csv(path, low_memory = False)\n",
    "\n",
    "    # Convert column type\n",
    "    df[\"author\"] = df[\"author\"].apply(str)\n",
    "\n",
    "    # Add column on removal status\n",
    "    df['removal_status'] = df.apply(categorize_comments, axis = 1)\n",
    "\n",
    "    # Add column on online status\n",
    "    df.loc[df[\"removal_status\"] == \"removed_by_moderator\", \"online\"] = 0\n",
    "    df.loc[df[\"removal_status\"] == \"deleted_by_user\", \"online\"] = 0\n",
    "    df.loc[df[\"removal_status\"] == \"online\", \"online\"] = 1\n",
    "    df[\"online\"] = df[\"online\"].astype(int)\n",
    "\n",
    "    y_prediction = df[\"label\"]\n",
    "    y_actual = df[\"online\"]\n",
    "\n",
    "    if prediction_target == \"misinformation\":\n",
    "        target_label = 0\n",
    "    elif prediction_target == \"truth\":\n",
    "        target_label = 1\n",
    "\n",
    "    # Calculate and print scores\n",
    "    print(f'Accuracy: {round(accuracy_score(y_actual, y_prediction) * 100, 2)}%')\n",
    "    print(f'Precision: {round(precision_score(y_actual, y_prediction, pos_label = target_label) * 100, 2)}%')\n",
    "    print(f'Recall: {round(recall_score(y_actual, y_prediction, pos_label = target_label) * 100, 2)}%')\n",
    "    print(f'F1 Score: {round(f1_score(y_actual, y_prediction, pos_label = target_label) * 100, 2)}%')\n",
    "\n",
    "    confusion_matrix_lr = pd.crosstab(y_prediction, y_actual)\n",
    "    display(confusion_matrix_lr)\n",
    "\n",
    "    # Show the unique values of every column\n",
    "    # for col in df:\n",
    "    #     print(col + \": \" + str(df[col].unique()))\n",
    "\n",
    "    # return df[[\"id\", \"author\", \"body\", \"score\", \"removal_status\", \"online\", \"label\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for submissions\n",
    "Metrics and the confusion matrix are shown for the submission results of each of the 4 subreddits separately, with three different results for each:\n",
    "* Results for the Random Forest model trained on Truthseeker that was applied in real time\n",
    "* Results for the Naive Bayes model trained on Fakeddit that was used afterwards\n",
    "* Results for the Neural Network model trained on Fakeddit that was used afterwards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### r/news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.14%\n",
      "Precision: 54.55%\n",
      "Recall: 31.58%\n",
      "F1 Score: 40.0%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online   0   1\n",
       "label         \n",
       "0        6   5\n",
       "1       13  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.14%\n",
      "Precision: 100.0%\n",
      "Recall: 5.26%\n",
      "F1 Score: 10.0%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit_nb_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online            0   1\n",
       "reddit_nb_label        \n",
       "0                 1   0\n",
       "1                18  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.38%\n",
      "Precision: 33.33%\n",
      "Recall: 5.26%\n",
      "F1 Score: 9.09%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit_nn_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online            0   1\n",
       "reddit_nn_label        \n",
       "0                 1   2\n",
       "1                18  21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyze_submissions(\"results/submissions_detailed_r_news.csv\", \"misinformation\")\n",
    "analyze_submissions(\"results/submissions_detailed_r_news.csv\", \"misinformation\", model = \"Reddit NB\")\n",
    "analyze_submissions(\"results/submissions_detailed_r_news.csv\", \"misinformation\", model = \"Reddit NN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### r/politics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.02%\n",
      "Precision: 65.17%\n",
      "Recall: 36.94%\n",
      "F1 Score: 47.15%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online   0   1\n",
       "label         \n",
       "0       58  31\n",
       "1       99  67"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.1%\n",
      "Precision: 68.89%\n",
      "Recall: 19.75%\n",
      "F1 Score: 30.69%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit_nb_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online             0   1\n",
       "reddit_nb_label         \n",
       "0                 31  14\n",
       "1                126  84"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 40.39%\n",
      "Precision: 64.71%\n",
      "Recall: 7.01%\n",
      "F1 Score: 12.64%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit_nn_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online             0   1\n",
       "reddit_nn_label         \n",
       "0                 11   6\n",
       "1                146  92"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyze_submissions(\"results/submissions_detailed_r_politics.csv\", \"misinformation\")\n",
    "analyze_submissions(\"results/submissions_detailed_r_politics.csv\", \"misinformation\", model = \"Reddit NB\")\n",
    "analyze_submissions(\"results/submissions_detailed_r_politics.csv\", \"misinformation\", model = \"Reddit NN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### r/science:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.07%\n",
      "Precision: 47.06%\n",
      "Recall: 80.0%\n",
      "F1 Score: 59.26%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online  0   1\n",
       "label        \n",
       "0       8   9\n",
       "1       2  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.17%\n",
      "Precision: 28.57%\n",
      "Recall: 20.0%\n",
      "F1 Score: 23.53%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit_nb_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online           0   1\n",
       "reddit_nb_label       \n",
       "0                2   5\n",
       "1                8  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.62%\n",
      "Precision: 0.0%\n",
      "Recall: 0.0%\n",
      "F1 Score: 0.0%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit_nn_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online            0   1\n",
       "reddit_nn_label        \n",
       "0                 0   2\n",
       "1                10  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyze_submissions(\"results/submissions_detailed_r_science.csv\", \"misinformation\")\n",
    "analyze_submissions(\"results/submissions_detailed_r_science.csv\", \"misinformation\", model = \"Reddit NB\")\n",
    "analyze_submissions(\"results/submissions_detailed_r_science.csv\", \"misinformation\", model = \"Reddit NN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### r/worldnews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.19%\n",
      "Precision: 42.47%\n",
      "Recall: 34.83%\n",
      "F1 Score: 38.27%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online   0   1\n",
       "label         \n",
       "0       31  42\n",
       "1       58  62"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.92%\n",
      "Precision: 66.67%\n",
      "Recall: 4.49%\n",
      "F1 Score: 8.42%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit_nb_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online            0    1\n",
       "reddit_nb_label         \n",
       "0                 4    2\n",
       "1                85  102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.96%\n",
      "Precision: 83.33%\n",
      "Recall: 5.62%\n",
      "F1 Score: 10.53%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit_nn_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online            0    1\n",
       "reddit_nn_label         \n",
       "0                 5    1\n",
       "1                84  103"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyze_submissions(\"results/submissions_detailed_r_worldnews.csv\", \"misinformation\")\n",
    "analyze_submissions(\"results/submissions_detailed_r_worldnews.csv\", \"misinformation\", model = \"Reddit NB\")\n",
    "analyze_submissions(\"results/submissions_detailed_r_worldnews.csv\", \"misinformation\", model = \"Reddit NN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for comments\n",
    "Metrics and the confusion matrix are shown for the comments results of each of the 4 subreddits. The results are based on the Neural Network model trained on Truthseeker that was applied in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.4%\n",
      "Precision: 5.9%\n",
      "Recall: 41.6%\n",
      "F1 Score: 10.34%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275</td>\n",
       "      <td>4385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>5897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online    0     1\n",
       "label            \n",
       "0       275  4385\n",
       "1       386  5897"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.52%\n",
      "Precision: 5.46%\n",
      "Recall: 48.47%\n",
      "F1 Score: 9.82%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>379</td>\n",
       "      <td>6558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>403</td>\n",
       "      <td>8310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online    0     1\n",
       "label            \n",
       "0       379  6558\n",
       "1       403  8310"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.08%\n",
      "Precision: 31.79%\n",
      "Recall: 45.14%\n",
      "F1 Score: 37.3%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>274</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>333</td>\n",
       "      <td>951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online    0    1\n",
       "label           \n",
       "0       274  588\n",
       "1       333  951"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.6%\n",
      "Precision: 6.72%\n",
      "Recall: 45.49%\n",
      "F1 Score: 11.71%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>429</td>\n",
       "      <td>5956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>514</td>\n",
       "      <td>8008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online    0     1\n",
       "label            \n",
       "0       429  5956\n",
       "1       514  8008"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyze_comments(\"results/comments_detailed_r_news.csv\", \"misinformation\")\n",
    "analyze_comments(\"results/comments_detailed_r_politics.csv\", \"misinformation\")\n",
    "analyze_comments(\"results/comments_detailed_r_science.csv\", \"misinformation\")\n",
    "analyze_comments(\"results/comments_detailed_r_worldnews.csv\", \"misinformation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The cases predicted by the \"label\" attribute and the facts captured by the \"online\" attribute are not fully the same - these are not predictions and actual values for one and the same (clear) target variable.\n",
    "\n",
    "The <b>ML model</b> predicts whether or not an input content can be classfied as misinformation (and <b>not</b> if such content, when posted on Reddit, is likely to be removed).\n",
    "\n",
    "The <b>real data</b> indicates whether or not a specific Reddit submission or comment was removed from the platform (for <b>any</b> reason, not just for being misinformation).\n",
    "\n",
    "The Truthseeker training dataset was very balanced quantitatively (approximately 50:50) and the predictions made by the trained model when it was applied to the stream were also quite balanced (but not 50:50) - this means that there is an enormous amount of false positives (content falsely classfied as misinformation).\n",
    "\n",
    "Across submissions and comments, accuracy (how much data was classified correctly overall) is between 50% and 60%, with recall (how much misinformation / deleted content was predicted correctly) typically going below 50%. Considerations that would otherwise possibly be made, e.g. discussing tradeoffs between recall and precision values and the related issue of finding as many target cases as possible vs. \"overshooting the target\", don't really make much sense here given the overall very low level of the results.\n",
    "\n",
    "To do a sanity check, we trained an additional model after looking at the initial results, based on \"Fakeddit\" (see notebook \"ml_model_training_fakeddit.ipynb\"). The Fakeddit training dataset was more unbalanced (after removing all image-related content) and thus more closely resembling our streamed Reddit data. However, while it showed superior performance (compared to the Truthseeker model) at detecting misinformation in its own testing dataset, the outcomes for our streamed Reddit submissions are much worse. This is a bit surprising, given that models trained on the Truthseeker data should in theory be more suitable for making predictions for our exact \"real data\" (see above), i.e. whether or not Reddit content gets removed.\n",
    "\n",
    "There might (must!) be aspects in our pipeline that can be improved, to create a better model. Even despite the difference in nature of the training data and our real data, the Truthseeker results were disappointing - at least at first, but then most of the Fakeddit results made them look good in comparison.\n",
    "\n",
    "<b>Main learnings:</b>\n",
    "\n",
    "1. Training an ML model to detect fake news / misinformation is complex. Better training data, better language preprocessing and better model fine-tuning would be needed.\n",
    "2. Tricking yourself or misleading the audience by not double-checking (or intentionally changing) the target of the evaluation metrics is really easy. In an unbalanced dataset with much more \"true\" than \"fake\" content - which is the case for real data like our Reddit stream data - setting the target to \"true\" (the default value for the sklearn.metrics functions is 1!) can easily yield 90% for some metrics even if hardly any misinformation is detected at all.\n",
    "3. Published datasets for ML training are not always what they appear to be at first sight. When reading through the associated papers (where available) and working through the dataset with pandas, we quickly realized that there are many assumptions and simplifications involved, that, while understandable, are certainly not helping the outcomes. There is no such thing as a \"gold standard\", general purpose, do-it-all misinformation dataset, no matter how many authors claim exactly that. The Fakeddit dataset we used mixes data about satire, fake news and manipulated image content, but we were unable to achieve good results using the satire and fake news parts of the dataset only. One of the reasons might be that there are no real \"fake\" or \"removed by moderator\" cases at all in the data, since the assumptions are basically: every Reddit post that is online after a year is \"true\", except for satire-related subreddits, where every post is \"fake\".\n",
    "4. Matching the training data more closely to the real data / use case, while necessary, might not be sufficient, as we suspect that the quantity of data would also need to be increased massively (see LLMs).\n",
    "\n",
    "Somewhat in line with the general topic of fake news and misinformation, here's an example for point 2 above, showing the actual results and a possible summary / headline someone could create out of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.52%\n",
      "Precision: 95.37%\n",
      "Recall: 55.89%\n",
      "F1 Score: 70.48%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>379</td>\n",
       "      <td>6558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>403</td>\n",
       "      <td>8310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online    0     1\n",
       "label            \n",
       "0       379  6558\n",
       "1       403  8310"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyze_comments(\"results/comments_detailed_r_politics.csv\", \"truth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\"We trained a Fake News detection Machine Learning model that has a precision of 95%\"</b>\n",
    "\n",
    "While technically not a complete lie, the precision refers to the detection of \"true\" data (or rather, content that did not get deleted after 1 week) and not misinformation or \"fake\" data (content that got deleted). At the same time, much more \"true\" data (14.868 instances) was present than \"fake\" data (782 instances) and out of the latter, only 379 instances were correctly detected. The high precision is purely pased on the low number of false positives (403) and is a given: if there are much less of the non-targeted cases, then there can't be many false positives either!\n",
    "\n",
    "$$ Precision = {True Positives \\over True Positives + False Positives} $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "Finally, we want to briefly explore some statistics to see if there are any differences in certain characteristics between removed and non-removed content. Below is the code we used to calculate means and medians for the scores and upvote ratios of content that was still online after a week and content that had been deleted by the user or removed by a moderator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics for submissions\n",
    "\n",
    "# Import and combine datasets\n",
    "df1 = pd.read_csv(\"results/submissions_detailed_r_news.csv\", low_memory = False)\n",
    "df2 = pd.read_csv(\"results/submissions_detailed_r_politics.csv\", low_memory = False)\n",
    "df3 = pd.read_csv(\"results/submissions_detailed_r_science.csv\", low_memory = False)\n",
    "df4 = pd.read_csv(\"results/submissions_detailed_r_worldnews.csv\", low_memory = False)\n",
    "df = pd.concat([df1, df2, df3, df4]).reset_index(drop = True)\n",
    "\n",
    "# Convert column types\n",
    "df[\"author\"] = df[\"author\"].apply(str)\n",
    "df[\"removed_by_category\"] = df[\"removed_by_category\"].apply(str)\n",
    "\n",
    "# Add column on removal status\n",
    "df['removal_status'] = df.apply(categorize_submissions, axis = 1)\n",
    "\n",
    "# Keep only relevant columns\n",
    "submissions_all = df[[\"id\", \"author\", \"selftext\", \"score\", \"upvote_ratio\", \"removal_status\", \"label\"]]\n",
    "\n",
    "# Calculate means and medians\n",
    "# submissions_all.loc[submissions_all[\"removal_status\"] == \"online\", \"score\"].mean()\n",
    "# submissions_all.loc[submissions_all[\"removal_status\"] == \"online\", \"score\"].median()\n",
    "# submissions_all.loc[submissions_all[\"removal_status\"] == \"online\", \"upvote_ratio\"].mean()\n",
    "# submissions_all.loc[submissions_all[\"removal_status\"] == \"online\", \"upvote_ratio\"].median()\n",
    "# submissions_all.loc[submissions_all[\"removal_status\"] == \"deleted_by_user\", \"score\"].mean()\n",
    "# submissions_all.loc[submissions_all[\"removal_status\"] == \"deleted_by_user\", \"score\"].median()\n",
    "# submissions_all.loc[submissions_all[\"removal_status\"] == \"deleted_by_user\", \"upvote_ratio\"].mean()\n",
    "# submissions_all.loc[submissions_all[\"removal_status\"] == \"deleted_by_user\", \"upvote_ratio\"].median()\n",
    "# submissions_all.loc[submissions_all[\"removal_status\"] == \"removed_by_moderator\", \"score\"].mean()\n",
    "# submissions_all.loc[submissions_all[\"removal_status\"] == \"removed_by_moderator\", \"score\"].median()\n",
    "# submissions_all.loc[submissions_all[\"removal_status\"] == \"removed_by_moderator\", \"upvote_ratio\"].mean()\n",
    "# submissions_all.loc[submissions_all[\"removal_status\"] == \"removed_by_moderator\", \"upvote_ratio\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics for comments\n",
    "\n",
    "# Import and combine datasets\n",
    "df1 = pd.read_csv(\"results/comments_detailed_r_news.csv\", low_memory = False)\n",
    "df2 = pd.read_csv(\"results/comments_detailed_r_politics.csv\", low_memory = False)\n",
    "df3 = pd.read_csv(\"results/comments_detailed_r_science.csv\", low_memory = False)\n",
    "df4 = pd.read_csv(\"results/comments_detailed_r_worldnews.csv\", low_memory = False)\n",
    "df = pd.concat([df1, df2, df3, df4]).reset_index(drop = True)\n",
    "\n",
    "# Convert column type\n",
    "df[\"author\"] = df[\"author\"].apply(str)\n",
    "\n",
    "# Add column on removal status\n",
    "df['removal_status'] = df.apply(categorize_comments, axis = 1)\n",
    "\n",
    "# Keep only relevant columns\n",
    "comments_all = df[[\"id\", \"author\", \"body\", \"score\", \"removal_status\",\"label\"]]\n",
    "\n",
    "# Calculate means and medians\n",
    "# comments_all.loc[comments_all[\"removal_status\"] == \"online\", \"score\"].mean()\n",
    "# comments_all.loc[comments_all[\"removal_status\"] == \"online\", \"score\"].median()\n",
    "# comments_all.loc[comments_all[\"removal_status\"] == \"deleted_by_user\", \"score\"].mean()\n",
    "# comments_all.loc[comments_all[\"removal_status\"] == \"deleted_by_user\", \"score\"].median()\n",
    "# comments_all.loc[comments_all[\"removal_status\"] == \"removed_by_moderator\", \"score\"].mean()\n",
    "# comments_all.loc[comments_all[\"removal_status\"] == \"removed_by_moderator\", \"score\"].median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions\n",
    "\n",
    "#### Online\n",
    "\n",
    "Average score: 2520\n",
    "\n",
    "Median score: 337\n",
    "\n",
    "Average upvote ratio: 0.87\n",
    "\n",
    "Median upvote ratio: 0.93\n",
    "\n",
    "#### Deleted by user\n",
    "\n",
    "Average score: 517\n",
    "\n",
    "Median score: 6\n",
    "\n",
    "Average upvote ratio: 0.85\n",
    "\n",
    "Median upvote ratio: 0.90\n",
    "\n",
    "#### Removed by moderator\n",
    "\n",
    "Average score: 494\n",
    "\n",
    "Median score: 5\n",
    "\n",
    "Average upvote ratio: 0.88\n",
    "\n",
    "Median upvote ratio: 0.97\n",
    "\n",
    "### Comments\n",
    "Note: there is no upvote ratio for comments, the score value is already the difference between upvotes and downvotes.\n",
    "\n",
    "#### Online\n",
    "\n",
    "Average (net) score: 22\n",
    "\n",
    "Median (net) score: 2\n",
    "\n",
    "#### Deleted by user\n",
    "\n",
    "Average (net) score: 12\n",
    "\n",
    "Median (net) score: 1\n",
    "\n",
    "#### Removed by moderator\n",
    "\n",
    "Average (net) score: 16\n",
    "\n",
    "Median (net) score: 1\n",
    "\n",
    "### Insights\n",
    "\n",
    "The difference in absolute score between deleted/removed submissions and those that remained online is very likely due to the additional time that the latter had to accumulate votes. The upvote ratio is almost identical between all three types. For comments, the scores are also surprisingly similar - content that ends up being deleted / removed can thus not be easily predicted by looking at low scores (obviously, doing this would also not be possible in real-time at the time of posting, since votes accumulate over time).\n",
    "\n",
    "These statistics indicate that score and upvote ratio, at least in most cases, are likely not suitable to make predictions. However, this is one of the assumptions made by the authors and creators of the \"Fakeddit\" dataset! In their view, content goes through multiple stages of filtering on Reddit, community voting being one of them.\n",
    "\n",
    "For submissions (but not so much for comments), the difference between average and median score is much higher for removed/deleted content than for online content, indicating that some very large submissions must have been taken down with quite some delay, allowing enough time to accumulate many votes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment\n",
    "Finally, it's time to revisit our initial project goals / research aspects:\n",
    "\n",
    "#### Content moderation:\n",
    "Based on the results, it is very clear that this specific model and method is not effective in filtering out misinformation and not a suitable replacement for human content moderation.\n",
    "\n",
    "#### Resource use:\n",
    "We saw no performance issues at all when streaming and classifying submissions and comments of 4 very large subreddits in real-time in 8 parallel Jupyter notebooks in Visual Studio Code locally on a Macbook Pro with a M1 Pro chip. With more resources, our approach should be feasible on a larger scale without issues, at least for Reddit, which has a lower volume of content than for example Twitter (which was evident in a Reddit/Twitter streaming project in the bachelor's studies).\n",
    "\n",
    "#### Reddit insights:\n",
    "Comparing content that remained online to content that was removed did not show any major statistical differences regarding score and upvote ratio. Manual inspection of both types of content showed that the model struggled to classify irony / sarcasm and other fine nuances.\n",
    "\n",
    "#### Model applicability:\n",
    "While the results based on a model trained on Twitter data were mediocre at best, the quick sanity check and comparison based on a model trained on Reddit data did not yield better results either (though we only applied it on submissions, which were very limited in number)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
